AWSTemplateFormatVersion: "2010-09-09"
Metadata:
    Generator: "Ram Nomula"
Description: "Script for creating resources to process client files"
Resources:
    EC2Instance:
        Type: "AWS::EC2::Instance"
        Properties:
            ImageId: "ami-0c02fb55956c7d316"
            InstanceType: "t2.micro"
            KeyName: "adobe_exercise_keypair"
            AvailabilityZone: !Sub "${AWS::Region}c"
            Tenancy: "default"
            SubnetId: "subnet-09a18fb184b2e3ebf"
            EbsOptimized: false
            SecurityGroupIds:
              - !Ref EC2SecurityGroup
            SourceDestCheck: true
            BlockDeviceMappings:
              -
                DeviceName: "/dev/xvda"
                Ebs:
                    Encrypted: false
                    VolumeSize: 8
                    # SnapshotId: "snap-0c1ac78aec1c4204c"
                    VolumeType: "gp2"
                    DeleteOnTermination: true
            Tags:
              -
                Key: "Name"
                Value: "linux-server-2"
            HibernationOptions:
                Configured: false
            EnclaveOptions:
                Enabled: false

    #Allocate an Elastic IP in your Account
    DemoElasticIP:
        Type: AWS::EC2::EIP
        Properties:
          Domain: vpc
          InstanceId: !Ref EC2Instance

    EC2Instance2:
        Type: "AWS::EC2::Instance"
        Properties:
            ImageId: "ami-056f71af0f2ca2daa"
            InstanceType: "m4.large"
            KeyName: "adobe_exercise_keypair"
            AvailabilityZone: !Sub "${AWS::Region}a"
            Tenancy: "default"
            SubnetId: "subnet-013501da42f49c7d6"
            EbsOptimized: true
            SecurityGroupIds:
              - !Ref EC2SecurityGroup
            SourceDestCheck: true
            BlockDeviceMappings:
              -
                DeviceName: "/dev/xvda"
                Ebs:
                    Encrypted: false
                    VolumeSize: 80
                    # SnapshotId: "snap-08550a3306b4c1258"
                    VolumeType: "gp3"
                    DeleteOnTermination: true
              -
                DeviceName: "/dev/sdb"
                Ebs:
                    Encrypted: false
                    VolumeSize: 150
                    # SnapshotId: ""
                    VolumeType: "gp2"
                    DeleteOnTermination: true
            Tags:
              -
                Key: "Name"
                Value: "gateway-server-2"
            HibernationOptions:
                Configured: false
            EnclaveOptions:
                Enabled: false

    # S3Bucket:
    #     Type: "AWS::S3::Bucket"
    #     Properties:
    #         BucketName: "adobe-s3-inbound"
    #         BucketEncryption:
    #             ServerSideEncryptionConfiguration:
    #               -
    #                 ServerSideEncryptionByDefault:
    #                     SSEAlgorithm: "AES256"
    #                 BucketKeyEnabled: false
    #         NotificationConfiguration:
    #             LambdaConfigurations:
    #               -
    #                 Event: "s3:ObjectCreated:Put"
    #                 Function: !GetAtt LambdaFunction.Arn

    EC2SecurityGroup:
        Type: "AWS::EC2::SecurityGroup"
        Properties:
            GroupDescription: "launch-wizard-1 created 2022-03-28T23:51:38.261-04:00"
            GroupName: "adobe_exercise_sg_2"
            VpcId: "vpc-03760ee22afe8df56"
            SecurityGroupIngress:
              -
                CidrIp: "0.0.0.0/0"
                FromPort: 80
                IpProtocol: "tcp"
                ToPort: 80
              -
                CidrIpv6: "::/0"
                FromPort: 80
                IpProtocol: "tcp"
                ToPort: 80
              -
                CidrIp: "0.0.0.0/0"
                Description: ""
                FromPort: 22
                IpProtocol: "tcp"
                ToPort: 22
              -
                CidrIpv6: "::/0"
                Description: ""
                FromPort: 22
                IpProtocol: "tcp"
                ToPort: 22
              -
                CidrIp: "0.0.0.0/0"
                FromPort: 443
                IpProtocol: "tcp"
                ToPort: 443
              -
                CidrIpv6: "::/0"
                FromPort: 443
                IpProtocol: "tcp"
                ToPort: 443
              -
                CidrIp: "0.0.0.0/0"
                FromPort: 2049
                IpProtocol: "tcp"
                ToPort: 2049
              -
                CidrIpv6: "::/0"
                FromPort: 2049
                IpProtocol: "tcp"
                ToPort: 2049
            SecurityGroupEgress:
              -
                CidrIp: "0.0.0.0/0"
                IpProtocol: "-1"

    LambdaFunction:
        Type: "AWS::Lambda::Function"
        Properties:
            Description: "Lambda function to trigger glue job."
            FunctionName: "adobe_exercise_lambda_glue_trigger_2"
            Handler: "lambda_function.lambda_handler"
            Architectures:
              - "x86_64"
            # Code:
            #     S3Bucket: "prod-04-2014-tasks"
            #     S3Key: !Sub "/snapshots/${AWS::AccountId}/adobe_exercise_lambda_glue_trigger-34cb1115-23af-4784-bd48-0f98afdcbe25"
            #     S3ObjectVersion: "mLgeRW_lkMOcduetxeoDiKUwMDR3RG1W"
            Code:
                ZipFile: |
                  import boto3
                  print('Loading function')

                  def lambda_handler(_event, _context):

                      glue = boto3.client('glue')
                      gluejobname = "adobe_exercise_glue_job"
                      
                      try:
                          runId = glue.start_job_run(JobName=gluejobname)
                          status = glue.get_job_run(JobName=gluejobname, RunId=runId['JobRunId'])
                          print("Job Status : ", status['JobRun']['JobRunState'])
                      except Exception as e:
                          print(e)
                          raise
            MemorySize: 128
            Role: !Sub "arn:aws:iam::${AWS::AccountId}:role/lambdaGlueTrigger"
            Runtime: "python3.7"
            Timeout: 3
            TracingConfig:
                Mode: "PassThrough"

    GlueJob:
        Type: "AWS::Glue::Job"
        Properties:
            Name: "adobe_exercise_glue_job_2"
            Description: "Glue job to process the files "
            Role: !Sub "arn:aws:iam::${AWS::AccountId}:role/adobeGlueServicerole"
            ExecutionProperty:
                MaxConcurrentRuns: 1
            Command:
                Name: "glueetl"
                ScriptLocation: !Sub "s3://aws-glue-assets-${AWS::AccountId}-${AWS::Region}/scripts/Untitled job.py"
                PythonVersion: "3"
            DefaultArguments:
                --TempDir: !Sub "s3://aws-glue-assets-${AWS::AccountId}-${AWS::Region}/temporary/"
                --class: "GlueApp"
                --enable-continuous-cloudwatch-log: "true"
                --enable-glue-datacatalog: "true"
                --enable-metrics: "true"
                --enable-spark-ui: "true"
                --job-bookmark-option: "job-bookmark-enable"
                --job-language: "python"
                --spark-event-logs-path: !Sub "s3://aws-glue-assets-${AWS::AccountId}-${AWS::Region}/sparkHistoryLogs/"
            MaxRetries: 0
            Timeout: 20
            GlueVersion: "3.0"
            NumberOfWorkers: 5
            WorkerType: "G.1X"